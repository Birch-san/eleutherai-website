{{ define "main" }}
<div class="main-container">
    <div class="content-wrapper">
        <div class="grid-1-1">
            <div class="page-content">
                {{ .Content }}
                {{/*  <h1>{{ with .Site.GetPage "section" "pile" }}{{ .Title }}{{ end }}</h1> this is for linking to project pages */}} 
                <div clas="promo-section">
                    <h4 class="promo-title">{{ .Params.promo_title}}</h4>
                    <h4 class="promo-text">{{ .Params.promo_text}}</h4>     
                </div>
                <div class="project-section">
                    <div class="project-icon"><img src="{{ .Site.BaseURL}}/images/gpt-neo.png" alt="gpt-neo"></div>
                    <div class="project-text"> <h5>GPT-Neo</h5>
                    <p>GPT-Neo is the name of our codebase for transformer-based language models loosely styled around the GPT architecture. One of our goals is to use GPT-Neo to replicate a GPT-3 sized model and open source it to the public, for free.
                    Along the way we will be running experiments with alternative architectures and attention types, releasing any intermediate models, and writing up any findings on our blog.
                    Our models are built in Mesh TensorFlow, which will allow us to scale up to GPT-3 sizes and beyond using simultaneous model and data parallelism.</p>
                    </div>
                </div>

                <div class="project-section">
                    <div class="project-icon"><img src="{{ .Site.BaseURL}}/images/the-pile.png" alt="the pile"></div>
                    <div class="project-text"> <h5>The Pile</h5>
                    <p>GPT-Neo is the name of our codebase for transformer-based language models loosely styled around the GPT architecture. One of our goals is to use GPT-Neo to replicate a GPT-3 sized model and open source it to the public, for free.
                    Along the way we will be running experiments with alternative architectures and attention types, releasing any intermediate models, and writing up any findings on our blog.
                    Our models are built in Mesh TensorFlow, which will allow us to scale up to GPT-3 sizes and beyond using simultaneous model and data parallelism.</p>
                    </div>
                </div>

                <div class="project-section">
                    <div class="project-icon"><img src="{{ .Site.BaseURL}}/images/open-web-text2.png" alt="open web text2"></div>
                    <div class="project-text"> <h5>OpenWebText2</h5>
                    <p>GPT-Neo is the name of our codebase for transformer-based language models loosely styled around the GPT architecture. One of our goals is to use GPT-Neo to replicate a GPT-3 sized model and open source it to the public, for free.
                    Along the way we will be running experiments with alternative architectures and attention types, releasing any intermediate models, and writing up any findings on our blog.
                    Our models are built in Mesh TensorFlow, which will allow us to scale up to GPT-3 sizes and beyond using simultaneous model and data parallelism.</p>
                    </div>
                </div>
            </div>
        </div>

    </div>
</div>
{{ end }}