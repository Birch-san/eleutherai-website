---
title: "Open Web Text 2"
date: 2019-04-26T20:18:54+03:00
layout: page
---

## Open Web Text 2

WebText is an internet dataset created by extracting URLs from Reddit submissions and scraping the URLs. It was collected for training the original GPT-2 and never released to the public, researchers independently reproduced the pipeline and released the resulting dataset, called OpenWebTextCorpus (OWT).

OpenWebText2 (OWT2) is an enhanced version of the original OpenWebTextCorpus covering all Reddit submissions from 2005 up until April 2020, with further months becoming available after the corresponding PushShift dump files are released.

OpenWebText2 is now live!
Download now, or you can read the docs

Comes pre-cleaned and pre-processed

Deduplicated by URL

Filtered by minimum combined reddit score 3

Deduplicated at document level with MinHashLSH.

Stats

17,103,059 documents

65.86 GB uncompressed text

28 GB compressed including text and metadata
