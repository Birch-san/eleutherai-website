---
title: "News & Announcements"
date: 2021-06-08T19:00:00-07:00
layout: page
---

## News & Announcements

## {class="content-block"}
- {{<date year="2021" month="06" day="08">}}
- **GPT⁠&#8288;-&#8288;J**  
GPT⁠&#8288;-&#8288;J-6B, a 6 billion parameter model trained on the Pile, is now available for use with our new codebase, Mesh Transformer JAX.  
[Mesh Transformer JAX on GitHub >](https://github.com/kingoflolz/mesh-transformer-jax/)

## {class="content-block"}
- {{<date year="2021" month="06" day="02">}}
- **Blog**  
We believe the creation and open source release of a large language model is a net good to AI safety. We explain why.  
[Why Release a Large Language Model? >](https://blog.eleuther.ai/why-release-a-large-language-model/)

## {class="content-block"}
- {{<date year="2021" month="04" day="20">}}
- **Blog**  
Rotary Positional Embedding (RoPE) is a new type of position encoding that unifies absolute and relative approaches. We put it to the test.  
[Rotary Embeddings: A Relative Revolution >](https://blog.eleuther.ai/rotary-embeddings/)

## {class="content-block"}
- {{<date year="2021" month="03" day="31">}}
- **GPT&#8288;-&#8288;Neo**  
GPT&#8288;-&#8288;Neo 1.3B and 2.7B are now available on Hugging Face Model Hub! Run the models with Transformers or call for them through their on-demand Inference API.  
[EleutherAI on Model Hub >](https://huggingface.co/EleutherAI)

## {class="content-block"}
- {{<date year="2021" month="03" day="21">}}
- **GPT&#8288;-&#8288;Neo**  
GPT&#8288;-&#8288;Neo 1.3B and 2.7B, trained on the Pile, are now available to run with the GPT&#8288;-&#8288;Neo framework.  
[GPT&#8288;-&#8288;Neo on GitHub >](https://github.com/EleutherAI/gpt-neo/)

## {class="content-block"}
- {{<date year="2021" month="01" day="01">}}
- **The Pile**  
We are proud to announce the release of the Pile, a free and publicly available 825GB dataset of diverse English text for language modeling!  
[Visit the Pile >](https://pile.eleuther.ai/)