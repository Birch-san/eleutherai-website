---
title: "GPT-Neo"
date: 2019-04-26T20:18:54+03:00
project_image: "images/the-pile.png"
layout: project-page
---

## {class="content-block"}
- ![alt](../../images/art49.png)
- # GPT-Neo 
    GPT&#8288;-&#8288;Neo is the code name for a family of transformer-based language models loosely styled around the GPT architecture. Our primary goal is to train an equivalent model to the full-sized GPT&#8288;-&#8288;3 and make it available to the public under an open licence.

    [GPT&#8288;-&#8288;Neo](https://github.com/EleutherAI/gpt-neo) is an implementation of model & data-parallel GPT&#8288;-&#8288;2 and GPT&#8288;-&#8288;3-like models, utilizing [Mesh&nbsp;Tensorflow](https://github.com/tensorflow/mesh) for distributed support. This codebase is designed for TPUs. It should also work on GPUs, though we do not recommend this hardware configuration.


## {class="content-block"}
- ### Progress:
    - GPT&#8288;-&#8288;Neo should be feature complete. We are making bugfixes, but we do not expect to make any significant changes. 
    - As of {{<date year="2021" month="03" day="21">}}, 1.3B and 2.7B parameter GPT&#8288;-&#8288;Neo models are available to be run with [GPT&#8288;-&#8288;Neo](https://github.com/EleutherAI/gpt-neo).
    - As of {{<date year="2021" month="03" day="31">}}, 1.3B and 2.7B parameter GPT&#8288;-&#8288;Neo models are [now available on Hugging Face Model Hub](https://huggingface.co/EleutherAI)!
- ### Next Steps:
    - We continue our efforts in in our GPU codebase, [GPT&#8288;-&#8288;NeoX](/projects/gpt-neox/).

